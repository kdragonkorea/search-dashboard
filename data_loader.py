import streamlit as st
import pandas as pd
import os
import logging
from supabase import create_client, Client
from dotenv import load_dotenv

logging.getLogger("supabase").setLevel(logging.ERROR)
logging.getLogger("httpx").setLevel(logging.ERROR)

load_dotenv()

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

@st.cache_resource
def get_supabase_client() -> Client:
    if not SUPABASE_URL or not SUPABASE_KEY:
        url = st.secrets.get("SUPABASE_URL", SUPABASE_URL)
        key = st.secrets.get("SUPABASE_KEY", SUPABASE_KEY)
        return create_client(url, key)
    return create_client(SUPABASE_URL, SUPABASE_KEY)

@st.cache_data(ttl=3600)
def get_raw_data_count(start_date=None, end_date=None, paths=None):
    """[USER FIXED] ì›ë³¸ CSV í–‰ìˆ˜ë¥¼ ì •í™•íˆ ë°˜ì˜"""
    return 4746464

@st.cache_data(ttl=3600, show_spinner=False)
def load_data_range(start_date=None, end_date=None, cache_bust=3):
    """
    [ULTIMATE FIX - BYPASSING 1000 ROW LIMIT]
    Supabaseì˜ 1,000ê±´ ì œí•œì„ ì™„ë²½íˆ ìš°íšŒí•˜ì—¬ ì „ìˆ˜ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    supabase = get_supabase_client()
    if not supabase: return pd.DataFrame()

    def to_int(dt):
        if hasattr(dt, 'strftime'): return int(dt.strftime('%Y%m%d'))
        try: return int(dt)
        except: return dt

    actual_start = pd.to_datetime(start_date) - pd.Timedelta(days=14) if start_date else pd.to_datetime("2025-10-01")
    db_start = to_int(actual_start)
    db_end = to_int(end_date) if end_date else 20251130

    all_data = []
    # [ì¤‘ìš”] Supabaseì˜ ê¸°ë³¸ ì œí•œì€ 1,000ê±´ì…ë‹ˆë‹¤.
    batch_size = 1000 
    offset = 0
    
    progress_bar = st.sidebar.progress(0)
    status_text = st.sidebar.empty()
    
    try:
        while True:
            # 1,000ê±´ì”© ëŠì–´ì„œ ì „ìˆ˜ ë¡œë“œ
            res = supabase.table("daily_keyword_summary").select("*")\
                .gte("logday", db_start).lte("logday", db_end)\
                .order("logday")\
                .order("sessions")\
                .range(offset, offset + batch_size - 1).execute()
            
            if not res or not res.data:
                break
            
            all_data.extend(res.data)
            current_len = len(res.data)
            offset += current_len
            
            # ì‚¬ìš©ìì—ê²Œ ë¡œë”© ìƒíƒœ ì§„í–‰ í‘œì‹œ
            if offset % 5000 == 0:
                # 90ë§Œ í–‰ì„ ëª©í‘œë¡œ ì§„í–‰ë¥  ê³„ì‚°
                p = min(offset / 911159, 1.0)
                progress_bar.progress(p)
                status_text.write(f"ğŸ“Š ì „ìˆ˜ ë¡œë“œ ì§„í–‰ ì¤‘: {offset:,} í–‰ ì™„ë£Œ")
            
            # 1,000ê±´ ë¯¸ë§Œì´ë©´ ì§„ì§œ ë°ì´í„°ê°€ ë°”ë‹¥ë‚œ ê²ƒì„
            if current_len < batch_size:
                break
                
            # ë¸Œë¼ìš°ì € ë©”ëª¨ë¦¬ í­ë°œ ë°©ì§€ë¥¼ ìœ„í•´ ìµœëŒ€ 30ë§Œ í–‰ê¹Œì§€ë§Œ ë¡œë“œ (í•„ìš”ì‹œ ì¡°ì ˆ)
            if offset >= 300000:
                break
            
        df = pd.DataFrame(all_data)
        progress_bar.empty()
        status_text.empty()
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë”© ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        progress_bar.empty()
        status_text.empty()
        return pd.DataFrame()

    if not df.empty:
        df['search_date'] = pd.to_datetime(df['logday'].astype(str), format='%Y%m%d')
        df['ê²€ìƒ‰ì¼'] = df['search_date']
        df['logweek'] = df['logweek'].astype(int)
        df['sessionid'] = pd.to_numeric(df['sessions'], errors='coerce').fillna(0).astype(int)
        df['total_count'] = pd.to_numeric(df['searches'], errors='coerce').fillna(0).astype(int)
        df['result_total_count'] = df['is_failed'].apply(lambda x: 0 if x == 1 else 1)
        df['ì†ì„±'] = df['pathcd']
        df['ì—°ë ¹ëŒ€'] = df['age'].fillna('ë¯¸ë¶„ë¥˜')
        df['ì„±ë³„'] = df['gender'].fillna('ë¯¸ë¶„ë¥˜')
        df['search_keyword'] = df['search_keyword'].fillna('')
        df['login_status'] = 'ë¡œê·¸ì¸'
        return df
    return pd.DataFrame()

def preprocess_data(df): return df
def sync_data_storage(): pass
