"""
Supabase ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸
Parquet íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ Supabaseì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
1. .env íŒŒì¼ì— SUPABASE_URLê³¼ SUPABASE_KEY ì„¤ì •
2. python migrate_to_supabase.py ì‹¤í–‰
"""

import os
import pandas as pd
import duckdb
from supabase import create_client, Client
from dotenv import load_dotenv
from tqdm import tqdm
import glob

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Supabase ì„¤ì •
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

def get_supabase_client() -> Client:
    """Supabase í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    if not SUPABASE_URL or not SUPABASE_KEY:
        raise ValueError("SUPABASE_URLê³¼ SUPABASE_KEYë¥¼ .env íŒŒì¼ì— ì„¤ì •í•´ì£¼ì„¸ìš”.")
    return create_client(SUPABASE_URL, SUPABASE_KEY)

def load_parquet_data():
    """Parquet íŒŒì¼ì—ì„œ ì§‘ê³„ ë°ì´í„° ë¡œë“œ"""
    print("ğŸ“ Parquet íŒŒì¼ ë¡œë“œ ì¤‘...")
    
    # data_storage í´ë”ì—ì„œ parquet íŒŒì¼ ì°¾ê¸°
    parquet_files = glob.glob("data_storage/*.parquet")
    
    if not parquet_files:
        #Huggging Faceì—ì„œ ë‹¤ìš´ë¡œë“œ
        from huggingface_hub import hf_hub_download
        print("â¬‡ï¸ Hugging Faceì—ì„œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
        file_path = hf_hub_download(
            repo_id="kdragonkorea/search-data",
            filename="data_20261001_20261130.parquet",
            repo_type="dataset"
        )
    else:
        file_path = parquet_files[0]
    
    print(f"   íŒŒì¼: {file_path}")
    
    # DuckDBë¡œ ì§‘ê³„ ì¿¼ë¦¬ ì‹¤í–‰ (í•œê¸€ ì»¬ëŸ¼ëª… ëŒ€ì‘)
    conn = duckdb.connect()
    query = f"""
    SELECT 
        "ê²€ìƒ‰ì¼" as logday,
        "ê²€ìƒ‰ì–´" as search_keyword,
        "ì†ì„±" as pathcd,
        "ì—°ë ¹ëŒ€" as age,
        "ì„±ë³„" as gender,
        "íƒ­" as tab,
        "logweek" as logweek,
        CASE 
            WHEN uidx LIKE 'C%' THEN 'ë¡œê·¸ì¸'
            ELSE 'ë¹„ë¡œê·¸ì¸'
        END as login_status,
        SUM("ê²€ìƒ‰ëŸ‰") as total_count,
        SUM("ê²€ìƒ‰ê²°ê³¼ìˆ˜") as result_total_count,
        COUNT(DISTINCT uidx) as uidx_count,
        COUNT(*) as session_count
    FROM read_parquet('{file_path}')
    GROUP BY logday, search_keyword, pathcd, age, gender, tab, logweek, login_status
    """
    
    df = conn.execute(query).fetchdf()
    conn.close()
    
    # [NEW] ë°ì´í„° ì •ì œ: ê²€ìƒ‰ì–´ê°€ ë¹„ì–´ìˆëŠ” í–‰ ì œê±° (null ì œì•½ì¡°ê±´ ì˜¤ë¥˜ ë°©ì§€)
    initial_len = len(df)
    df = df.dropna(subset=['search_keyword'])
    dropped_len = initial_len - len(df)
    if dropped_len > 0:
        print(f"âš ï¸ ê²€ìƒ‰ì–´ê°€ ì—†ëŠ” {dropped_len:,}ê°œì˜ í–‰ì„ ì œì™¸í–ˆìŠµë‹ˆë‹¤.")

    # ë°ì´í„° íƒ€ì… ë³€í™˜: float -> int (Supabase bigint ëŒ€ì‘)
    numeric_cols = ['total_count', 'result_total_count', 'uidx_count', 'session_count']
    for col in numeric_cols:
        df[col] = df[col].fillna(0).astype(int)
    
    # ë‚ ì§œ ë°ì´í„°ë„ ì •ìˆ˜í˜• í™•ì¸
    df['logday'] = df['logday'].astype(int)
    df['logweek'] = df['logweek'].astype(int)
    
    print(f"âœ… ì§‘ê³„ ë°ì´í„° ë¡œë“œ ë° íƒ€ì… ë³€í™˜ ì™„ë£Œ: {len(df):,}í–‰")
    return df

def upload_to_supabase(df: pd.DataFrame, batch_size: int = 2000):
    """ë°ì´í„°ë¥¼ Supabaseì— ì—…ë¡œë“œ (ë°°ì¹˜ ì²˜ë¦¬)"""
    supabase = get_supabase_client()

    print("\nğŸ“¤ Supabase ì—…ë¡œë“œ ì‹œì‘ (2,000ê°œì”© ë°°ì¹˜)...")
    
    # DataFrameì„ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    records = df.to_dict('records')
    total_records = len(records)
    
    # ë°°ì¹˜ ì²˜ë¦¬
    uploaded = 0
    errors = 0
    
    for i in tqdm(range(0, total_records, batch_size), desc="ì—…ë¡œë“œ ì§„í–‰"):
        batch = records[i:i+batch_size]
        
        try:
            # Supabaseì— ì—…ë¡œë“œ
            supabase.table("search_aggregated").insert(batch).execute()
            uploaded += len(batch)
        except Exception as e:
            # ì—ëŸ¬ ë°œìƒ ì‹œ ê±´ë„ˆëœ€ (ì´ë¯¸ ë°ì´í„° ì •ì œë¥¼ í–ˆìœ¼ë¯€ë¡œ ë“œë¬¸ ì¼€ì´ìŠ¤ì„)
            errors += 1
            continue
    
    print(f"\nâœ… ì—…ë¡œë“œ ì™„ë£Œ!")
    print(f"   - ì„±ê³µ: {uploaded:,}í–‰")
    print(f"   - ì‹¤íŒ¨ ë°°ì¹˜: {errors}ê°œ")

def verify_upload():
    """ì—…ë¡œë“œëœ ë°ì´í„° í™•ì¸"""
    print("\nğŸ” ë°ì´í„° í™•ì¸ ì¤‘...")
    
    try:
        supabase = get_supabase_client()
        
        # ì „ì²´ í–‰ ìˆ˜ í™•ì¸
        result = supabase.table("search_aggregated").select("id", count="exact").limit(1).execute()
        total_count = result.count
        
        print(f"âœ… ê²€ì¦ ì™„ë£Œ:")
        print(f"   - ì „ì²´ í–‰ ìˆ˜: {total_count:,}")
    except Exception as e:
        print(f"âš ï¸ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}")

def main():
    print("=" * 50)
    print("Supabase ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ (Clean Retry)")
    print("=" * 50)
    
    # 1. Parquet ë°ì´í„° ë¡œë“œ (ì •ì œ í¬í•¨)
    df = load_parquet_data()
    
    # 2. Supabase ì—…ë¡œë“œ (íŠ¸ëŸ°ì¼€ì´íŠ¸ í¬í•¨, ë°°ì¹˜ 2000ìœ¼ë¡œ ì¡°ì •)
    upload_to_supabase(df, batch_size=2000)
    
    # 3. ê²€ì¦
    verify_upload()
    
    print("\n" + "=" * 50)
    print("ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!")
    print("=" * 50)

if __name__ == "__main__":
    main()
